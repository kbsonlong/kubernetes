diff --git a/.gitignore b/.gitignore
index 42bad0847ac..de1b35148e5 100644
--- a/.gitignore
+++ b/.gitignore
@@ -123,6 +123,3 @@ zz_generated_*_test.go
 
 # generated by verify-vendor.sh
 vendordiff.patch
-config/*
-
-go.work
\ No newline at end of file
diff --git a/OWNERS_ALIASES b/OWNERS_ALIASES
new file mode 100644
index 00000000000..6ae260fcb11
--- /dev/null
+++ b/OWNERS_ALIASES
@@ -0,0 +1,516 @@
+aliases:
+  # Note: sig-architecture-approvers has approval on root files (including go.mod/go.sum) until https://github.com/kubernetes/test-infra/pull/21398 is resolved.
+  # People with approve rights via this alias should defer dependency update PRs to dep-approvers.
+  sig-architecture-approvers:
+    - dims
+    - derekwaynecarr
+    - johnbelamaric
+  # sig-auth subproject aliases
+  sig-auth-audit-approvers:
+    - sttts
+    - tallclair
+  sig-auth-audit-reviewers:
+    - hzxuzhonghu
+    - lavalamp
+    - sttts
+    - tallclair
+  sig-auth-authenticators-approvers:
+    - deads2k
+    - liggitt
+    - mikedanese
+    - enj
+  sig-auth-authenticators-reviewers:
+    - deads2k
+    - enj
+    - lavalamp
+    - liggitt
+    - mikedanese
+    - sttts
+    - wojtek-t
+  sig-auth-authorizers-approvers:
+    - deads2k
+    - liggitt
+    - mikedanese
+  sig-auth-authorizers-reviewers:
+    - deads2k
+    - dims
+    - enj
+    - lavalamp
+    - liggitt
+    - mikedanese
+    - ncdc
+    - smarterclayton
+    - sttts
+    - thockin
+    - wojtek-t
+  sig-auth-certificates-approvers:
+    - liggitt
+    - mikedanese
+    - smarterclayton
+  sig-auth-certificates-reviewers:
+    - caesarxuchao
+    - deads2k
+    - dims
+    - enj
+    - lavalamp
+    - liggitt
+    - mikedanese
+    - smarterclayton
+    - sttts
+    - thockin
+    - wojtek-t
+  sig-auth-encryption-at-rest-approvers:
+    - smarterclayton
+    - enj
+  sig-auth-encryption-at-rest-reviewers:
+    - enj
+    - lavalamp
+    - liggitt
+    - smarterclayton
+    - wojtek-t
+  sig-auth-node-isolation-approvers:
+    - deads2k
+    - liggitt
+    - mikedanese
+    - tallclair
+  sig-auth-node-isolation-reviewers:
+    - deads2k
+    - liggitt
+    - mikedanese
+    - tallclair
+  sig-auth-policy-approvers:
+    - deads2k
+    - liggitt
+    - tallclair
+  sig-auth-policy-reviewers:
+    - deads2k
+    - liggitt
+    - tallclair
+    - krmayankk
+  sig-auth-serviceaccounts-approvers:
+    - deads2k
+    - liggitt
+    - mikedanese
+  sig-auth-serviceaccounts-reviewers:
+    - deads2k
+    - enj
+    - liggitt
+    - mikedanese
+  # SIG Release
+  release-engineering-approvers:
+    - cpanato # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - jeremyrickard # SIG Chair / RelEng subproject owner / Release Manager
+    - justaugustus # SIG Chair / RelEng subproject owner / Release Manager
+    - puerco # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - saschagrunert # SIG Chair / RelEng subproject owner / Release Manager
+    - Verolop # SIG Technical Lead / RelEng subproject owner / Release Manager
+  release-managers:
+    - cpanato
+    - jeremyrickard
+    - justaugustus
+    - palnabarun
+    - puerco
+    - saschagrunert
+    - Verolop
+    - xmudrii
+  build-image-approvers:
+    - BenTheElder
+    - cblecker
+    - cpanato # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - dims
+    - jeremyrickard # SIG Chair / RelEng subproject owner / Release Manager
+    - justaugustus # SIG Chair / RelEng subproject owner / Release Manager
+    - palnabarun # Release Manager
+    - puerco # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - saschagrunert # SIG Chair / RelEng subproject owner / Release Manager
+    - Verolop # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - xmudrii # Release Manager
+  build-image-reviewers:
+    - BenTheElder
+    - cblecker
+    - cpanato # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - dims
+    - jeremyrickard # SIG Chair / RelEng subproject owner / Release Manager
+    #- justaugustus # SIG Chair / RelEng subproject owner / Release Manager - approvals only
+    - palnabarun # Release Manager
+    - puerco # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - saschagrunert # SIG Chair / RelEng subproject owner / Release Manager
+    - Verolop # SIG Technical Lead / RelEng subproject owner / Release Manager
+    - xmudrii # Release Manager
+  sig-storage-approvers:
+    - gnufied
+    - jsafrane
+    - msau42
+    - saad-ali
+    - thockin
+    - xing-yang
+  # emeritus:
+  # - rootfs
+  sig-storage-reviewers:
+    - chrishenzie
+    - gnufied
+    - humblec
+    - jsafrane
+    - jingxu97
+    - mattcary
+    - mauriciopoppe
+    - msau42
+    - saikat-royc
+    - xing-yang
+  # emeritus:
+  # - davidz627
+  # - Jiawei0227
+  # - rootfs
+  # - verult
+  sig-scheduling-maintainers:
+    - alculquicondor
+    - Huang-Wei
+    - ahg-g
+    - damemi
+  # emeritus:
+  # - bsalamat
+  # - k82cn
+  # - ravisantoshgudimetla
+  # - wojtek-t
+  sig-scheduling:
+    - chendave
+    - denkensk
+    - sanposhiho
+    - kerthcet
+  # emeritus:
+  # - adtac
+  # - liu-cong
+  # - draveness
+  # - hex108
+  # - resouer
+  # - wgliang
+
+  sig-cli-maintainers:
+    - ardaguclu
+    - apelisse
+    - brianpursley
+    - deads2k
+    - eddiezane
+    - KnVerey
+    - pwittrock
+    - seans3
+    - soltysh
+  # emeritus:
+  # - adohe
+  # - brendandburns
+  # - droot
+  # - janetkuo
+  # - mengqiy
+  # - monopole
+  # - smarterclayton
+  sig-cli-reviewers:
+    - ardaguclu
+    - brianpursley
+    - eddiezane
+    - KnVerey
+    - mpuckett159
+    - seans3
+    - soltysh
+  sig-testing-reviewers:
+    - bentheelder
+    - cblecker
+    - spiffxp
+    - dims
+  sig-node-approvers:
+    - Random-Liu
+    - dchen1107
+    - derekwaynecarr
+    - yujuhong
+    - sjenning
+    - mrunalp
+    - klueska
+  # emeretus:
+  # - dashpole
+  # - vishh
+  sig-node-reviewers:
+    - Random-Liu
+    - dchen1107
+    - derekwaynecarr
+    - dims
+    - endocrimes
+    - feiskyer
+    - mtaufen
+    - sjenning
+    - wzshiming
+    - yujuhong
+    - krmayankk
+    - matthyx
+    - odinuge
+    - andrewsykim
+    - mrunalp
+    - SergeyKanzhelev
+    - bobbypage
+    - pacoxu
+    - bart0sh
+  sig-network-driver-approvers:
+    - dcbw
+    - freehan
+  sig-network-approvers:
+    - andrewsykim
+    - aojea
+    - bowei
+    - caseydavenport
+    - danwinship
+    - dcbw
+    - freehan
+    - khenidak
+    - mrhohn
+    - robscott
+    - thockin
+  sig-network-reviewers:
+    - andrewsykim
+    - aojea
+    - bowei
+    - caseydavenport
+    - danwinship
+    - dcbw
+    - freehan
+    - khenidak
+    - mrhohn
+    - robscott
+    - thockin
+  sig-apps-approvers:
+    - kow3ns
+    - janetkuo
+    - soltysh
+    - smarterclayton
+  sig-apps-reviewers:
+    - alculquicondor
+    - atiratree
+    - janetkuo
+    - kow3ns
+    - krmayankk
+    - mortent
+    - smarterclayton
+    - soltysh
+  # sig-apps-emeritus:
+  # - tnozicka
+
+  sig-autoscaling-maintainers:
+    - bskiba
+    - MaciekPytel
+    - mwielgus
+  sig-instrumentation-approvers:
+    - logicalhan
+    - dashpole
+    - ehashman
+    - RainbowMango
+    - serathius
+    - dgrisonnet
+  sig-instrumentation-reviewers:
+    - dashpole
+    - ehashman
+    - s-urbaniak
+    - coffeepac
+    - logicalhan
+    - RainbowMango
+    - serathius
+    - dgrisonnet
+    - pohly
+  # sig-instrumentation-emeritus
+  # - brancz
+  # - DirectXMan12
+
+  api-approvers:
+    - lavalamp
+    - smarterclayton
+    - thockin
+    - liggitt
+  # subsets of api-approvers by sig area to help focus approval requests to those with domain knowledge
+  sig-api-machinery-api-approvers:
+    - lavalamp
+    - liggitt
+    - smarterclayton
+  sig-apps-api-approvers:
+    - lavalamp
+    - liggitt
+    - smarterclayton
+  sig-auth-api-approvers:
+    - liggitt
+    - smarterclayton
+  sig-cli-api-approvers:
+    - liggitt
+    - smarterclayton
+  sig-cloud-provider-api-approvers:
+    - liggitt
+    - thockin
+  sig-cluster-lifecycle-api-approvers:
+    - liggitt
+    - smarterclayton
+  sig-cluster-lifecycle-leads:
+    - fabriziopandini
+    - justinsb
+    - neolit123
+    - vincepri
+  sig-network-api-approvers:
+    - smarterclayton
+    - thockin
+  sig-network-api-reviewers:
+    - andrewsykim
+    - caseydavenport
+    - danwinship
+    - thockin
+  sig-node-api-approvers:
+    - smarterclayton
+    - thockin
+  sig-scheduling-api-approvers:
+    - lavalamp
+    - smarterclayton
+    - thockin
+  sig-security-approvers:
+    - IanColdwater
+    - tabbysable
+  sig-security-reviewers:
+    - IanColdwater
+    - tabbysable
+  sig-storage-api-approvers:
+    - liggitt
+    - thockin
+  sig-windows-api-approvers:
+    - smarterclayton
+    - thockin
+    - liggitt
+  api-reviewers:
+    - andrewsykim
+    - lavalamp
+    - smarterclayton
+    - thockin
+    - liggitt
+    - wojtek-t
+    - deads2k
+    - yujuhong
+    - derekwaynecarr
+    - caesarxuchao
+    - mikedanese
+    - sttts
+    - dchen1107
+    - saad-ali
+    - luxas
+    - janetkuo
+    - justinsb
+    - pwittrock
+    - ncdc
+    - tallclair
+    - mwielgus
+    - soltysh
+    - jsafrane
+    - dims
+    - cici37
+  # api-reviewers targeted by sig area
+  # see https://git.k8s.io/community/sig-architecture/api-review-process.md#training-reviews
+  sig-api-machinery-api-reviewers:
+    - caesarxuchao
+    - deads2k
+    - jpbetz
+    - sttts
+    - cici37
+  sig-apps-api-reviewers:
+    - janetkuo
+    - kow3ns
+    - soltysh
+    - alculquicondor
+  sig-auth-api-reviewers:
+    - enj
+    - mikedanese
+  sig-cli-api-reviewers:
+    - pwittrock
+    - soltysh
+  sig-cloud-provider-api-reviewers:
+    - andrewsykim
+    - cheftako
+    - dims
+  # sig-cluster-lifecycle-api-reviewers:
+  #   -
+  #   -
+  sig-contributor-experience-approvers:
+    - mrbobbytables
+    - cblecker
+    - nikhita
+  sig-docs-approvers:
+    - jimangel
+    - kbhawkey
+    - onlydole
+    - sftim
+  sig-node-api-reviewers:
+    - dchen1107
+    - derekwaynecarr
+    - tallclair
+    - yujuhong
+  sig-scalability-approvers:
+    - marseel
+    - mborsz
+    - wojtek-t
+  # emeritus:
+  # - mm4tt
+  sig-scalability-reviewers:
+    - marseel
+    - mborsz
+    - wojtek-t
+  # emeritus:
+  # - mm4tt
+  sig-scheduling-api-reviewers:
+    - alculquicondor
+  sig-storage-api-reviewers:
+    - deads2k
+    - saad-ali
+    - msau42
+    - jsafrane
+    - xing-yang
+  sig-windows-api-reviewers:
+    - jayunit100
+    - jsturtevant
+    - marosset
+  # Note: dep-approvers has approval on root files (including OWNERS_ALIASES) until https://github.com/kubernetes/test-infra/pull/21398 is resolved.
+  # People with approve rights via this alias should defer updates of root files other than go.mod/go.sum to dep-approvers.
+  dep-approvers:
+    - BenTheElder
+    - cblecker
+    - dims
+    - thockin
+    - sttts
+    - soltysh
+    - liggitt
+  dep-reviewers:
+    - logicalhan
+  feature-approvers:
+    - andrewsykim # Cloud Provider
+    - logicalhan # Instrumentation
+    - ahg-g # Scheduling
+    - caseydavenport # Network
+    - dcbw # Network
+    - dchen1107 # Node
+    - deads2k # API Machinery
+    - derekwaynecarr # Node
+    - dims # Architecture
+    - ehashman # Instrumentation
+    - janetkuo # Apps
+    - jayunit100 # Windows
+    - jsafrane # Storage
+    - jsturtevant # Windows
+    - kow3ns # Apps
+    - lavalamp # API Machinery
+    - liggitt # Auth
+    - luxas # Cluster Lifecycle
+    - marosset # Windows
+    - msau42 # Storage
+    - mwielgus # Autoscaling
+    - pwittrock # CLI
+    - justinsb # Cluster Lifecycle
+    - saad-ali # Storage
+    - seans3 # CLI
+    - soltysh # Apps, CLI
+    - tallclair # Auth
+    - thockin # Network
+    - xing-yang # Storage
+    - wojtek-t # Scalability
+  # conformance aliases https://git.k8s.io/enhancements/keps/sig-architecture/20190412-conformance-behaviors.md
+  conformance-behavior-approvers:
+    - smarterclayton
+    - johnbelamaric
+    - spiffxp
+    - dims
diff --git a/cmd/kube-controller-manager/app/controllermanager.go b/cmd/kube-controller-manager/app/controllermanager.go
index 6a9d2a1e6c9..0b503c88b83 100644
--- a/cmd/kube-controller-manager/app/controllermanager.go
+++ b/cmd/kube-controller-manager/app/controllermanager.go
@@ -102,7 +102,6 @@ const (
 
 // NewControllerManagerCommand creates a *cobra.Command object with default parameters
 func NewControllerManagerCommand() *cobra.Command {
-	// 初始化controllerManager的参数,例如ServiceControllerOptions、DaemonSetControllerOptions等
 	s, err := options.NewKubeControllerManagerOptions()
 	if err != nil {
 		klog.Background().Error(err, "Unable to initialize command options")
@@ -135,14 +134,13 @@ controller, and serviceaccounts controller.`,
 				return err
 			}
 			cliflag.PrintFlags(cmd.Flags())
-			// 获取所有控制器配置,默认禁用的控制器
+
 			c, err := s.Config(KnownControllers(), ControllersDisabledByDefault.List())
 			if err != nil {
 				return err
 			}
 			// add feature enablement metrics
 			utilfeature.DefaultMutableFeatureGate.AddMetrics()
-			// 启动控制器
 			return Run(context.Background(), c.Complete())
 		},
 		Args: func(cmd *cobra.Command, args []string) error {
@@ -181,7 +179,6 @@ func ResyncPeriod(c *config.CompletedConfig) func() time.Duration {
 }
 
 // Run runs the KubeControllerManagerOptions.
-// 这段代码负责启动和管理kube-controller-manager的控制器，并处理健康检查和Leader选举。
 func Run(ctx context.Context, c *config.CompletedConfig) error {
 	logger := klog.FromContext(ctx)
 	stopCh := ctx.Done()
@@ -192,7 +189,6 @@ func Run(ctx context.Context, c *config.CompletedConfig) error {
 	logger.Info("Golang settings", "GOGC", os.Getenv("GOGC"), "GOMAXPROCS", os.Getenv("GOMAXPROCS"), "GOTRACEBACK", os.Getenv("GOTRACEBACK"))
 
 	// Start events processing pipeline.
-	// 启动事件处理进程
 	c.EventBroadcaster.StartStructuredLogging(0)
 	c.EventBroadcaster.StartRecordingToSink(&v1core.EventSinkImpl{Interface: c.Client.CoreV1().Events("")})
 	defer c.EventBroadcaster.Shutdown()
@@ -232,14 +228,12 @@ func Run(ctx context.Context, c *config.CompletedConfig) error {
 	saTokenControllerInitFunc := serviceAccountTokenControllerStarter{rootClientBuilder: rootClientBuilder}.startServiceAccountTokenController
 
 	run := func(ctx context.Context, startSATokenController InitFunc, initializersFunc ControllerInitializersFunc) {
-		// 创建controller的context
 		controllerContext, err := CreateControllerContext(logger, c, rootClientBuilder, clientBuilder, ctx.Done())
 		if err != nil {
 			logger.Error(err, "Error building controller context")
 			klog.FlushAndExit(klog.ExitFlushTimeout, 1)
 		}
 		controllerInitializers := initializersFunc(controllerContext.LoopMode)
-		// 启动各个控制器
 		if err := StartControllers(ctx, controllerContext, startSATokenController, controllerInitializers, unsecuredMux, healthzHandler); err != nil {
 			logger.Error(err, "Error starting controllers")
 			klog.FlushAndExit(klog.ExitFlushTimeout, 1)
@@ -253,7 +247,6 @@ func Run(ctx context.Context, c *config.CompletedConfig) error {
 	}
 
 	// No leader election, run directly
-	// 不选举直接运行上面 `run` 函数
 	if !c.ComponentConfig.Generic.LeaderElection.LeaderElect {
 		run(ctx, saTokenControllerInitFunc, NewControllerInitializers)
 		return nil
@@ -265,7 +258,6 @@ func Run(ctx context.Context, c *config.CompletedConfig) error {
 	}
 
 	// add a uniquifier so that two processes on the same host don't accidentally both become active
-	// 添加唯一标识符，以便同一主机上的两个进程不会意外同时激活
 	id = id + "_" + string(uuid.NewUUID())
 
 	// leaderMigrator will be non-nil if and only if Leader Migration is enabled.
@@ -275,7 +267,6 @@ func Run(ctx context.Context, c *config.CompletedConfig) error {
 	startSATokenController := saTokenControllerInitFunc
 
 	// If leader migration is enabled, create the LeaderMigrator and prepare for migration
-	// 如果启用了leader迁移，则创建LeaderMigrator并准备迁移
 	if leadermigration.Enabled(&c.ComponentConfig.Generic) {
 		logger.Info("starting leader migration")
 
@@ -291,13 +282,11 @@ func Run(ctx context.Context, c *config.CompletedConfig) error {
 	}
 
 	// Start the main lock
-	// Leader选举获取锁
 	go leaderElectAndRun(ctx, c, id, electionChecker,
 		c.ComponentConfig.Generic.LeaderElection.ResourceLock,
 		c.ComponentConfig.Generic.LeaderElection.ResourceName,
 		leaderelection.LeaderCallbacks{
 			OnStartedLeading: func(ctx context.Context) {
-				// 实例化各控制器初始化函数
 				initializersFunc := NewControllerInitializers
 				if leaderMigrator != nil {
 					// If leader migration is enabled, we should start only non-migrated controllers
@@ -410,7 +399,6 @@ type ControllerInitializersFunc func(loopMode ControllerLoopMode) (initializers
 var _ ControllerInitializersFunc = NewControllerInitializers
 
 // KnownControllers returns all known controllers's name
-// 返回所有控制器名称
 func KnownControllers() []string {
 	ret := sets.StringKeySet(NewControllerInitializers(IncludeCloudLoops))
 
@@ -426,7 +414,6 @@ func KnownControllers() []string {
 }
 
 // ControllersDisabledByDefault is the set of controllers which is disabled by default
-// 默认禁用的控制器
 var ControllersDisabledByDefault = sets.NewString(
 	"bootstrapsigner",
 	"tokencleaner",
@@ -438,12 +425,10 @@ const (
 
 // NewControllerInitializers is a public map of named controller groups (you can start more than one in an init func)
 // paired to their InitFunc.  This allows for structured downstream composition and subdivision.
-// 定义了各种controller的类型和其对于的启动函数
 func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc {
 	controllers := map[string]InitFunc{}
 
 	// All of the controllers must have unique names, or else we will explode.
-	// 通过调用register函数来注册各个控制器的初始化函数。每个控制器都有一个唯一的名称，并且会检查是否有重复的名称。
 	register := func(name string, fn InitFunc) {
 		if _, found := controllers[name]; found {
 			panic(fmt.Sprintf("controller name %q was registered twice", name))
@@ -451,7 +436,6 @@ func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc
 		controllers[name] = fn
 	}
 
-	// 注册各个控制器初始化函数，比如endpoint、deployment 、 replicaset、pv
 	register("endpoint", startEndpointController)
 	register("endpointslice", startEndpointSliceController)
 	register("endpointslicemirroring", startEndpointSliceMirroringController)
@@ -463,7 +447,6 @@ func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc
 	register("garbagecollector", startGarbageCollectorController)
 	register("daemonset", startDaemonSetController)
 	register("job", startJobController)
-	// Deployment Controller 资源初始化
 	register("deployment", startDeploymentController)
 	register("replicaset", startReplicaSetController)
 	register("horizontalpodautoscaling", startHPAController)
@@ -484,17 +467,11 @@ func NewControllerInitializers(loopMode ControllerLoopMode) map[string]InitFunc
 		register("cloud-node-lifecycle", startCloudNodeLifecycleController)
 		// TODO: volume controller into the IncludeCloudLoops only set.
 	}
-	// in tree pv 控制器
 	register("persistentvolume-binder", startPersistentVolumeBinderController)
-	// attachdetach控制器负责处理容器的挂载和卸载操作。
-	// 它使用CSI（Container Storage Interface）插件来管理持久卷的挂载和卸载。
 	register("attachdetach", startAttachDetachController)
-	// out of tree pv 控制器
 	register("persistentvolume-expander", startVolumeExpandController)
 	register("clusterrole-aggregation", startClusterRoleAggregrationController)
-	// PVC保护控制器
 	register("pvc-protection", startPVCProtectionController)
-	// PV保护控制器
 	register("pv-protection", startPVProtectionController)
 	register("ttl-after-finished", startTTLAfterFinishedController)
 	register("root-ca-cert-publisher", startRootCACertPublisher)
@@ -542,10 +519,8 @@ func GetAvailableResources(clientBuilder clientbuilder.ControllerClientBuilder)
 // CreateControllerContext creates a context struct containing references to resources needed by the
 // controllers such as the cloud provider and clientBuilder. rootClientBuilder is only used for
 // the shared-informers client and token controller.
-// 构建了各种controller所需的资源的上下文，各种controller在启动时，入参为该context，具体参考initFn(ctx)。
 func CreateControllerContext(logger klog.Logger, s *config.CompletedConfig, rootClientBuilder, clientBuilder clientbuilder.ControllerClientBuilder, stop <-chan struct{}) (ControllerContext, error) {
 	versionedClient := rootClientBuilder.ClientOrDie("shared-informers")
-	// 创建SharedInformerFactory
 	sharedInformers := informers.NewSharedInformerFactory(versionedClient, ResyncPeriod(s)())
 
 	metadataClient := metadata.NewForConfigOrDie(rootClientBuilder.ConfigOrDie("metadata-informers"))
@@ -569,14 +544,13 @@ func CreateControllerContext(logger klog.Logger, s *config.CompletedConfig, root
 	if err != nil {
 		return ControllerContext{}, err
 	}
-	// createCloudProvider 整合云提供商需要的东西，明确列出云提供商需要的东西作为参数
+
 	cloud, loopMode, err := createCloudProvider(logger, s.ComponentConfig.KubeCloudShared.CloudProvider.Name, s.ComponentConfig.KubeCloudShared.ExternalCloudVolumePlugin,
 		s.ComponentConfig.KubeCloudShared.CloudProvider.CloudConfigFile, s.ComponentConfig.KubeCloudShared.AllowUntaggedCloud, sharedInformers)
 	if err != nil {
 		return ControllerContext{}, err
 	}
 
-	// 赋值给ControllerContext
 	ctx := ControllerContext{
 		ClientBuilder:                   clientBuilder,
 		InformerFactory:                 sharedInformers,
@@ -624,9 +598,7 @@ func StartControllers(ctx context.Context, controllerCtx ControllerContext, star
 	//   so we cannot rely on it yet to add the name
 	// - it allows distinguishing between log entries emitted by the controller
 	//   and those emitted for it - this is a bit debatable and could be revised.
-	// 依次启动注册的控制器
 	for controllerName, initFn := range controllers {
-		// 检查控制器是否启用
 		if !controllerCtx.IsControllerEnabled(controllerName) {
 			logger.Info("Warning: controller is disabled", "controller", controllerName)
 			continue
@@ -635,7 +607,6 @@ func StartControllers(ctx context.Context, controllerCtx ControllerContext, star
 		time.Sleep(wait.Jitter(controllerCtx.ComponentConfig.Generic.ControllerStartInterval.Duration, ControllerStartJitter))
 
 		logger.V(1).Info("Starting controller", "controller", controllerName)
-		// 开始启动控制器
 		ctrl, started, err := initFn(klog.NewContext(ctx, klog.LoggerWithName(logger, controllerName)), controllerCtx)
 		if err != nil {
 			logger.Error(err, "Error starting controller", "controller", controllerName)
diff --git a/cmd/kube-controller-manager/app/core.go b/cmd/kube-controller-manager/app/core.go
index 1b4e8b6baab..2049ecdda60 100644
--- a/cmd/kube-controller-manager/app/core.go
+++ b/cmd/kube-controller-manager/app/core.go
@@ -78,7 +78,6 @@ const (
 	defaultNodeMaskCIDRIPv6 = 64
 )
 
-// serivce控制器
 func startServiceController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) {
 	serviceController, err := servicecontroller.New(
 		controllerContext.Cloud,
@@ -249,7 +248,6 @@ func startRouteController(ctx context.Context, controllerContext ControllerConte
 	return nil, true, nil
 }
 
-// 启动持久卷绑定控制器的逻辑，它负责管理持久卷和持久卷声明之间的绑定关系
 func startPersistentVolumeBinderController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) {
 	logger := klog.FromContext(ctx)
 	plugins, err := ProbeControllerVolumePlugins(logger, controllerContext.Cloud, controllerContext.ComponentConfig.PersistentVolumeBinderController.VolumeConfiguration)
@@ -286,11 +284,9 @@ func startPersistentVolumeBinderController(ctx context.Context, controllerContex
 
 func startAttachDetachController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) {
 	logger := klog.FromContext(ctx)
-	// 实例化csiNode、csiDriver对象
 	csiNodeInformer := controllerContext.InformerFactory.Storage().V1().CSINodes()
 	csiDriverInformer := controllerContext.InformerFactory.Storage().V1().CSIDrivers()
 
-	// 探测可用的存储卷插件
 	plugins, err := ProbeAttachableVolumePlugins(logger)
 	if err != nil {
 		return nil, true, fmt.Errorf("failed to probe volume plugins when starting attach/detach controller: %v", err)
@@ -303,8 +299,6 @@ func startAttachDetachController(ctx context.Context, controllerContext Controll
 		return nil, true, err
 	}
 
-	// 监听Pod、Nodeport、PVC、PV和VolumeAttachments,
-	// 并调用csi 插件来执行挂载、卸载操作
 	ctx = klog.NewContext(ctx, logger)
 	attachDetachController, attachDetachControllerErr :=
 		attachdetach.NewAttachDetachController(
@@ -328,21 +322,17 @@ func startAttachDetachController(ctx context.Context, controllerContext Controll
 	if attachDetachControllerErr != nil {
 		return nil, true, fmt.Errorf("failed to start attach/detach controller: %v", attachDetachControllerErr)
 	}
-	// 启动attachDetach控制器
 	go attachDetachController.Run(ctx)
 	return nil, true, nil
 }
 
 func startVolumeExpandController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) {
 	logger := klog.FromContext(ctx)
-	// 探测可使用的外部存储卷(csi)插件
 	plugins, err := ProbeExpandableVolumePlugins(logger, controllerContext.ComponentConfig.PersistentVolumeBinderController.VolumeConfiguration)
 	if err != nil {
 		return nil, true, fmt.Errorf("failed to probe volume plugins when starting volume expand controller: %v", err)
 	}
-	// 实例化csitrans对象
 	csiTranslator := csitrans.New()
-	// 解析允许访问白名单CIDR
 	filteredDialOptions, err := options.ParseVolumeHostFilters(
 		controllerContext.ComponentConfig.PersistentVolumeBinderController.VolumeHostCIDRDenylist,
 		controllerContext.ComponentConfig.PersistentVolumeBinderController.VolumeHostAllowLocalLoopback)
@@ -559,7 +549,6 @@ func startGarbageCollectorController(ctx context.Context, controllerContext Cont
 	return garbageCollector, true, nil
 }
 
-// PVC保护控制器
 func startPVCProtectionController(ctx context.Context, controllerContext ControllerContext) (controller.Interface, bool, error) {
 	pvcProtectionController, err := pvcprotection.NewPVCProtectionController(
 		klog.FromContext(ctx),
diff --git a/cmd/kube-controller-manager/app/options/options.go b/cmd/kube-controller-manager/app/options/options.go
index ffa5b2490f9..6c322f92521 100644
--- a/cmd/kube-controller-manager/app/options/options.go
+++ b/cmd/kube-controller-manager/app/options/options.go
@@ -96,7 +96,6 @@ type KubeControllerManagerOptions struct {
 }
 
 // NewKubeControllerManagerOptions creates a new KubeControllerManagerOptions with a default config.
-// 使用默认配置创建一个新的 KubeControllerManagerOptions
 func NewKubeControllerManagerOptions() (*KubeControllerManagerOptions, error) {
 	componentConfig, err := NewDefaultComponentConfig()
 	if err != nil {
diff --git a/cmd/kube-controller-manager/controller-manager.go b/cmd/kube-controller-manager/controller-manager.go
index 09ddb19cd4c..77bc10a3517 100644
--- a/cmd/kube-controller-manager/controller-manager.go
+++ b/cmd/kube-controller-manager/controller-manager.go
@@ -1,12 +1,3 @@
-/*
- * @FilePath: /Users/zengshenglong/Code/GoWorkSpace/kubernetes/cmd/kube-controller-manager/controller-manager.go
- * @Author: kbsonlong kbsonlong@gmail.com
- * @Date: 2022-11-17 11:23:17
- * @LastEditors: kbsonlong kbsonlong@gmail.com
- * @LastEditTime: 2023-07-28 17:35:19
- * @Description:
- * Copyright (c) 2023 by kbsonlong, All Rights Reserved.
- */
 /*
 Copyright 2014 The Kubernetes Authors.
 
@@ -45,8 +36,3 @@ func main() {
 	code := cli.Run(command)
 	os.Exit(code)
 }
-
-// 调用流程:
-// Main --> NewControllerManagerCommand --> Run(context.Background(), c.Complete()) \
-//      --> run --> StartControllers --> initFn(ctx) --> startDeploymentController/startStatefulSetController \
-//      --> sts.NewStatefulSetController.Run/dc.NewDeploymentController.Run --> pkg/controller
diff --git a/cmd/kube-scheduler/app/server.go b/cmd/kube-scheduler/app/server.go
index 722e4e4f715..07de3418ece 100644
--- a/cmd/kube-scheduler/app/server.go
+++ b/cmd/kube-scheduler/app/server.go
@@ -137,7 +137,6 @@ func runCommand(cmd *cobra.Command, opts *options.Options, registryOptions ...Op
 		cancel()
 	}()
 
-	// 初始化 scheduler 调度器对象
 	cc, sched, err := Setup(ctx, opts, registryOptions...)
 	if err != nil {
 		return err
@@ -148,7 +147,6 @@ func runCommand(cmd *cobra.Command, opts *options.Options, registryOptions ...Op
 }
 
 // Run executes the scheduler based on the given configuration. It only returns on error or when context is done.
-// scheduler 的选举实现
 func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *scheduler.Scheduler) error {
 	logger := klog.FromContext(ctx)
 
@@ -197,7 +195,6 @@ func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *
 	}
 
 	// Start all informers.
-	// 启动 informers， 这里只有 pod 和 node
 	cc.InformerFactory.Start(ctx.Done())
 	// DynInformerFactory can be nil in tests.
 	if cc.DynInformerFactory != nil {
@@ -205,7 +202,6 @@ func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *
 	}
 
 	// Wait for all caches to sync before scheduling.
-	// 同步 informer 的数据到本地缓存
 	cc.InformerFactory.WaitForCacheSync(ctx.Done())
 	// DynInformerFactory can be nil in tests.
 	if cc.DynInformerFactory != nil {
@@ -213,18 +209,14 @@ func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *
 	}
 
 	// If leader election is enabled, runCommand via LeaderElector until done and exit.
-	// 如果在配置中启动了选举, 创建选举对象, 注册事件方法, 并启用选举.
 	if cc.LeaderElection != nil {
 		cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks{
 			OnStartedLeading: func(ctx context.Context) {
 				close(waitingForLeader)
-				// 当选举拿到 leader 时, 启动 scheduler 调度器
 				sched.Run(ctx)
 			},
 			OnStoppedLeading: func() {
 				select {
-				// 当选举成功但后面又丢失 leader 后, 则退出进程.
-				// 进程退出后, 会被 docker 或 systemd 重新拉起, 尝试拿锁.
 				case <-ctx.Done():
 					// We were asked to terminate. Exit 0.
 					logger.Info("Requested to terminate, exiting")
@@ -236,13 +228,11 @@ func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *
 				}
 			},
 		}
-		// 构建 leaderelection 对象
 		leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)
 		if err != nil {
 			return fmt.Errorf("couldn't create leader elector: %v", err)
 		}
 
-		// 启动选举
 		leaderElector.Run(ctx)
 
 		return fmt.Errorf("lost lease")
@@ -250,7 +240,6 @@ func Run(ctx context.Context, cc *schedulerserverconfig.CompletedConfig, sched *
 
 	// Leader election is disabled, so runCommand inline until done.
 	close(waitingForLeader)
-	// scheudler 启动入口
 	sched.Run(ctx)
 	return fmt.Errorf("finished without leader elect")
 }
@@ -318,14 +307,12 @@ func WithPlugin(name string, factory runtime.PluginFactory) Option {
 
 // Setup creates a completed config and a scheduler based on the command args and options
 func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions ...Option) (*schedulerserverconfig.CompletedConfig, *scheduler.Scheduler, error) {
-	// 获取默认配置
 	if cfg, err := latest.Default(); err != nil {
 		return nil, nil, err
 	} else {
 		opts.ComponentConfig = cfg
 	}
 
-	// 验证 scheduler 的配置参数
 	if errs := opts.Validate(); len(errs) > 0 {
 		return nil, nil, utilerrors.NewAggregate(errs)
 	}
@@ -336,7 +323,6 @@ func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions
 	}
 
 	// Get the completed config
-	// 配置填充和调整
 	cc := c.Complete()
 
 	outOfTreeRegistry := make(runtime.Registry)
@@ -349,7 +335,6 @@ func Setup(ctx context.Context, opts *options.Options, outOfTreeRegistryOptions
 	recorderFactory := getRecorderFactory(&cc)
 	completedProfiles := make([]kubeschedulerconfig.KubeSchedulerProfile, 0)
 	// Create the scheduler.
-	// 创建 scheduler 对象
 	sched, err := scheduler.New(cc.Client,
 		cc.InformerFactory,
 		cc.DynInformerFactory,
diff --git a/pkg/controller/deployment/deployment_controller.go b/pkg/controller/deployment/deployment_controller.go
index 2b112a525ef..c08dd0c1908 100644
--- a/pkg/controller/deployment/deployment_controller.go
+++ b/pkg/controller/deployment/deployment_controller.go
@@ -27,7 +27,7 @@ import (
 	"time"
 
 	apps "k8s.io/api/apps/v1"
-	v1 "k8s.io/api/core/v1"
+	"k8s.io/api/core/v1"
 	"k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/labels"
@@ -98,7 +98,6 @@ type DeploymentController struct {
 }
 
 // NewDeploymentController creates a new DeploymentController.
-// Deployment Controller 资源初始化
 func NewDeploymentController(ctx context.Context, dInformer appsinformers.DeploymentInformer, rsInformer appsinformers.ReplicaSetInformer, podInformer coreinformers.PodInformer, client clientset.Interface) (*DeploymentController, error) {
 	eventBroadcaster := record.NewBroadcaster()
 	logger := klog.FromContext(ctx)
@@ -211,9 +210,6 @@ func (dc *DeploymentController) deleteDeployment(logger klog.Logger, obj interfa
 	dc.enqueueDeployment(d)
 }
 
-// 根据ReplicaSet ownerReferences 寻找到对应的Deployment Name
-// 判断是否Rs 发生了变化
-// 如果变化就把Deployment 塞到Wokrer Queue里面去
 // addReplicaSet enqueues the deployment that manages a ReplicaSet when the ReplicaSet is created.
 func (dc *DeploymentController) addReplicaSet(logger klog.Logger, obj interface{}) {
 	rs := obj.(*apps.ReplicaSet)
diff --git a/pkg/scheduler/eventhandlers.go b/pkg/scheduler/eventhandlers.go
index 5b969669322..95ed57114b5 100644
--- a/pkg/scheduler/eventhandlers.go
+++ b/pkg/scheduler/eventhandlers.go
@@ -248,8 +248,6 @@ func responsibleForPod(pod *v1.Pod, profiles profile.Map) bool {
 
 // addAllEventHandlers is a helper function used in tests and in Scheduler
 // to add event handlers for various informers.
-// 注册在 scheduler informer 的回调方法
-// 在 informer 里注册 pod 和 node 资源的回调方法，监听 pod 事件对 queue 和 cache 做回调处理. 监听 node 事件对 cache 做处理.
 func addAllEventHandlers(
 	sched *Scheduler,
 	informerFactory informers.SharedInformerFactory,
@@ -257,7 +255,6 @@ func addAllEventHandlers(
 	gvkMap map[framework.GVK]framework.ActionType,
 ) {
 	// scheduled pod cache
-	// 监听 pod 事件，并注册增删改回调方法，其操作是对 cache 的增删改
 	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
 		cache.FilteringResourceEventHandler{
 			FilterFunc: func(obj interface{}) bool {
@@ -285,7 +282,6 @@ func addAllEventHandlers(
 		},
 	)
 	// unscheduled pod queue
-	// 监听 pod 事件，并注册增删改方法, 对 queue 插入增删改事件.
 	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
 		cache.FilteringResourceEventHandler{
 			FilterFunc: func(obj interface{}) bool {
@@ -313,7 +309,6 @@ func addAllEventHandlers(
 		},
 	)
 
-	// 监听 node 事件，注册回调方法，该方法在 cache 里对 node 的增删改查.
 	informerFactory.Core().V1().Nodes().Informer().AddEventHandler(
 		cache.ResourceEventHandlerFuncs{
 			AddFunc:    sched.addNodeToCache,
diff --git a/pkg/scheduler/internal/queue/scheduling_queue.go b/pkg/scheduler/internal/queue/scheduling_queue.go
index 56d084f4de0..50fc17d63ce 100644
--- a/pkg/scheduler/internal/queue/scheduling_queue.go
+++ b/pkg/scheduler/internal/queue/scheduling_queue.go
@@ -288,7 +288,6 @@ func newQueuedPodInfoForLookup(pod *v1.Pod, plugins ...string) *framework.Queued
 }
 
 // NewPriorityQueue creates a PriorityQueue object.
-// 实例化优先级队列，该队列中含有 activeQ, podBackoffQ, unschedulablePods集合.
 func NewPriorityQueue(
 	lessFn framework.LessFunc,
 	informerFactory informers.SharedInformerFactory,
@@ -396,32 +395,26 @@ func (p *PriorityQueue) addToActiveQ(pInfo *framework.QueuedPodInfo) (bool, erro
 
 // Add adds a pod to the active queue. It should be called only when a new pod
 // is added so there is no chance the pod is already in active/unschedulable/backoff queues
-// 添加 pod 对象
-
 func (p *PriorityQueue) Add(pod *v1.Pod) error {
 	p.lock.Lock()
 	defer p.lock.Unlock()
 
 	pInfo := p.newQueuedPodInfo(pod)
 	gated := pInfo.Gated
-	// 把对象加到 activeQ 队列里.
 	if added, err := p.addToActiveQ(pInfo); !added {
 		return err
 	}
-	// 如果该对象在不可调度集合中存在, 则需要在里面删除.
 	if p.unschedulablePods.get(pod) != nil {
 		klog.ErrorS(nil, "Error: pod is already in the unschedulable queue", "pod", klog.KObj(pod))
 		p.unschedulablePods.delete(pod, gated)
 	}
 	// Delete pod from backoffQ if it is backing off
-	// 从 backoffQ 删除 pod 对象
 	if err := p.podBackoffQ.Delete(pInfo); err == nil {
 		klog.ErrorS(nil, "Error: pod is already in the podBackoff queue", "pod", klog.KObj(pod))
 	}
 	klog.V(5).InfoS("Pod moved to an internal scheduling queue", "pod", klog.KObj(pod), "event", PodAdd, "queue", activeQName)
 	metrics.SchedulerQueueIncomingPods.WithLabelValues("active", PodAdd).Inc()
 	p.addNominatedPodUnlocked(pInfo.PodInfo, nil)
-	// 条件变量通知
 	p.cond.Broadcast()
 
 	return nil
@@ -508,11 +501,9 @@ func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodI
 		return fmt.Errorf("Pod %v is already present in unschedulable queue", klog.KObj(pod))
 	}
 
-	// 去重判断
 	if _, exists, _ := p.activeQ.Get(pInfo); exists {
 		return fmt.Errorf("Pod %v is already present in the active queue", klog.KObj(pod))
 	}
-	// 去重判断
 	if _, exists, _ := p.podBackoffQ.Get(pInfo); exists {
 		return fmt.Errorf("Pod %v is already present in the backoff queue", klog.KObj(pod))
 	}
@@ -525,7 +516,6 @@ func (p *PriorityQueue) AddUnschedulableIfNotPresent(pInfo *framework.QueuedPodI
 	for plugin := range pInfo.UnschedulablePlugins {
 		metrics.UnschedulableReason(plugin, pInfo.Pod.Spec.SchedulerName).Inc()
 	}
-	// 把没有调度成功的 podInfo 扔到 backoffQ 队列或者 unschedulablePods 集合中.
 	if p.moveRequestCycle >= podSchedulingCycle {
 		if err := p.podBackoffQ.Add(pInfo); err != nil {
 			return fmt.Errorf("error adding pod %v to the backoff queue: %v", klog.KObj(pod), err)
@@ -600,11 +590,9 @@ func (p *PriorityQueue) flushUnschedulablePodsLeftover() {
 // Pop removes the head of the active queue and returns it. It blocks if the
 // activeQ is empty and waits until a new item is added to the queue. It
 // increments scheduling cycle when a pod is popped.
-// 获取 pod info 对象
 func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
 	p.lock.Lock()
 	defer p.lock.Unlock()
-	// 如果 activeQ 为空, 陷入等待
 	for p.activeQ.Len() == 0 {
 		// When the queue is empty, invocation of Pop() is blocked until new item is enqueued.
 		// When Close() is called, the p.closed is set and the condition is broadcast,
@@ -614,8 +602,6 @@ func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
 		}
 		p.cond.Wait()
 	}
-
-	// 从 activeQ 堆顶 pop 对象
 	obj, err := p.activeQ.Pop()
 	if err != nil {
 		return nil, err
@@ -899,7 +885,6 @@ func (npm *nominator) NominatedPodsForNode(nodeName string) []*framework.PodInfo
 	return pods
 }
 
-// heap 的比较方法，确保 deadline 最低的在 heap 顶部.
 func (p *PriorityQueue) podsCompareBackoffCompleted(podInfo1, podInfo2 interface{}) bool {
 	pInfo1 := podInfo1.(*framework.QueuedPodInfo)
 	pInfo2 := podInfo2.(*framework.QueuedPodInfo)
@@ -931,7 +916,6 @@ func (p *PriorityQueue) getBackoffTime(podInfo *framework.QueuedPodInfo) time.Ti
 
 // calculateBackoffDuration is a helper function for calculating the backoffDuration
 // based on the number of attempts the pod has made.
-// backoff duration 随着重试次数不断叠加，但最大不能超过 maxBackoffDuration.
 func (p *PriorityQueue) calculateBackoffDuration(podInfo *framework.QueuedPodInfo) time.Duration {
 	duration := p.podInitialBackoffDuration
 	for i := 1; i < podInfo.Attempts; i++ {
diff --git a/pkg/scheduler/schedule_one.go b/pkg/scheduler/schedule_one.go
index 7d773b5f51a..90e2d30e9c9 100644
--- a/pkg/scheduler/schedule_one.go
+++ b/pkg/scheduler/schedule_one.go
@@ -60,7 +60,6 @@ const (
 
 // scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting.
 func (sched *Scheduler) scheduleOne(ctx context.Context) {
-	// 从 activeQ 中获取需要调度的 pod 数据
 	podInfo := sched.NextPod()
 	// pod could be nil when schedulerQueue is closed
 	if podInfo == nil || podInfo.Pod == nil {
@@ -92,16 +91,13 @@ func (sched *Scheduler) scheduleOne(ctx context.Context) {
 	schedulingCycleCtx, cancel := context.WithCancel(ctx)
 	defer cancel()
 
-	// 为 pod 选择最优的 node 节点
 	scheduleResult, assumedPodInfo, status := sched.schedulingCycle(schedulingCycleCtx, state, fwk, podInfo, start, podsToActivate)
 	if !status.IsSuccess() {
-		// 如何没有找到节点，则执行失败方法.
 		sched.FailureHandler(schedulingCycleCtx, fwk, assumedPodInfo, status, scheduleResult.nominatingInfo, start)
 		return
 	}
 
 	// bind the pod to its host asynchronously (we can do this b/c of the assumption step above).
-	// 像 apiserver 发起 pod -> node 绑定
 	go func() {
 		bindingCycleCtx, cancel := context.WithCancel(ctx)
 		defer cancel()
@@ -121,10 +117,6 @@ func (sched *Scheduler) scheduleOne(ctx context.Context) {
 var clearNominatedNode = &framework.NominatingInfo{NominatingMode: framework.ModeOverride, NominatedNodeName: ""}
 
 // schedulingCycle tries to schedule a single Pod.
-// schedulingCycle() 该方法主要为 pod 选出最优的 node 节点.
-// 先通过预选过程过滤出符合 pod 要求的节点集合,
-// 再通过插件对这些节点进行打分,
-// 使用分值最高的 node 为 pod 调度节点
 func (sched *Scheduler) schedulingCycle(
 	ctx context.Context,
 	state *framework.CycleState,
@@ -134,7 +126,6 @@ func (sched *Scheduler) schedulingCycle(
 	podsToActivate *framework.PodsToActivate,
 ) (ScheduleResult, *framework.QueuedPodInfo, *framework.Status) {
 	pod := podInfo.Pod
-	// 选择节点
 	scheduleResult, err := sched.SchedulePod(ctx, fwk, state, pod)
 	if err != nil {
 		if err == ErrNoNodesAvailable {
@@ -181,7 +172,6 @@ func (sched *Scheduler) schedulingCycle(
 	assumedPodInfo := podInfo.DeepCopy()
 	assumedPod := assumedPodInfo.Pod
 	// assume modifies `assumedPod` by setting NodeName=scheduleResult.SuggestedHost
-	// 在缓存 cache 中更新状态
 	err = sched.assume(assumedPod, scheduleResult.SuggestedHost)
 	if err != nil {
 		// This is most probably result of a BUG in retrying logic.
@@ -341,17 +331,10 @@ func (sched *Scheduler) skipPodSchedule(fwk framework.Framework, pod *v1.Pod) bo
 // schedulePod tries to schedule the given pod to one of the nodes in the node list.
 // If it succeeds, it will return the name of the node.
 // If it fails, it will return a FitError with reasons.
-// scheduler 内置各个阶段的各种插件, 预选和优选阶段就是遍历回调插件求出结果.
-// 调度周期 schedulingCycle 内关键方法是 schedulePod, 其简化流程如下.
-// 1. 先调用 findNodesThatFitPod 过滤出符合要求的预选节点;
-// 2. 调用 prioritizeNodes 为预选出来的节点进行打分 score;
-// 3. 最后调用 selectHost 选择最合适的 node 节点.
-
 func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
 	trace := utiltrace.New("Scheduling", utiltrace.Field{Key: "namespace", Value: pod.Namespace}, utiltrace.Field{Key: "name", Value: pod.Name})
 	defer trace.LogIfLong(100 * time.Millisecond)
 
-	// 更新快照
 	if err := sched.Cache.UpdateSnapshot(sched.nodeInfoSnapshot); err != nil {
 		return result, err
 	}
@@ -361,14 +344,12 @@ func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework
 		return result, ErrNoNodesAvailable
 	}
 
-	// 进行预选筛选
 	feasibleNodes, diagnosis, err := sched.findNodesThatFitPod(ctx, fwk, state, pod)
 	if err != nil {
 		return result, err
 	}
 	trace.Step("Computing predicates done")
 
-	// 预选下来，无节点可以用, 返回错误
 	if len(feasibleNodes) == 0 {
 		return result, &framework.FitError{
 			Pod:         pod,
@@ -378,7 +359,6 @@ func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework
 	}
 
 	// When only one node after predicate, just use it.
-	// 只有一个节点通过筛选，那么直接返回
 	if len(feasibleNodes) == 1 {
 		return ScheduleResult{
 			SuggestedHost:  feasibleNodes[0].Name,
@@ -387,13 +367,11 @@ func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework
 		}, nil
 	}
 
-	// 进行优选过程, 对预选出来的节点进行打分
 	priorityList, err := prioritizeNodes(ctx, sched.Extenders, fwk, state, pod, feasibleNodes)
 	if err != nil {
 		return result, err
 	}
 
-	// 从优选中选出最高分的节点
 	host, err := selectHost(priorityList)
 	trace.Step("Prioritizing done")
 
@@ -406,23 +384,19 @@ func (sched *Scheduler) schedulePod(ctx context.Context, fwk framework.Framework
 
 // Filters the nodes to find the ones that fit the pod based on the framework
 // filter plugins and filter extenders.
-// findNodesThatFitPod 方法用来实现调度器的预选过程, 其内部调用插件的 PreFilter 和 Filter 方法来筛选出符合 pod 要求的 node 节点集合.
 func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
 	diagnosis := framework.Diagnosis{
 		NodeToStatusMap:      make(framework.NodeToStatusMap),
 		UnschedulablePlugins: sets.NewString(),
 	}
 
-	// 获取所有的 nodes
 	allNodes, err := sched.nodeInfoSnapshot.NodeInfos().List()
 	if err != nil {
 		return nil, diagnosis, err
 	}
 	// Run "prefilter" plugins.
-	// 调用 framework 的 PreFilter 集合里的插件
 	preRes, s := fwk.RunPreFilterPlugins(ctx, state, pod)
 	if !s.IsSuccess() {
-		// 如果在 prefilter 有异常, 则直接跳出.
 		if !s.IsUnschedulable() {
 			return nil, diagnosis, s.AsError()
 		}
@@ -450,7 +424,6 @@ func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.F
 		}
 	}
 
-	// 根据 prefilter 拿到的 node names 获取 node info 对象
 	nodes := allNodes
 	if !preRes.AllNodes() {
 		nodes = make([]*framework.NodeInfo, 0, len(preRes.NodeNames))
@@ -462,9 +435,6 @@ func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.F
 			nodes = append(nodes, nInfo)
 		}
 	}
-
-	// 运行 framework 的 filter 插件判断 node 是否可以运行新 pod
-	// scheduleOne -> findNodesThatFitPod -> findNodesThatPassesFilters
 	feasibleNodes, err := sched.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, nodes)
 	// always try to update the sched.nextStartNodeIndex regardless of whether an error has occurred
 	// this is helpful to make sure that all the nodes have a chance to be searched
@@ -474,7 +444,6 @@ func (sched *Scheduler) findNodesThatFitPod(ctx context.Context, fwk framework.F
 		return nil, diagnosis, err
 	}
 
-	// 调用额外的 extender 调度器来进行预选
 	feasibleNodes, err = findNodesThatPassExtenders(sched.Extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
 	if err != nil {
 		return nil, diagnosis, err
@@ -510,18 +479,13 @@ func (sched *Scheduler) findNodesThatPassFilters(
 	pod *v1.Pod,
 	diagnosis framework.Diagnosis,
 	nodes []*framework.NodeInfo) ([]*v1.Node, error) {
-
 	numAllNodes := len(nodes)
-	// 计算需要扫描的 nodes 数, 避免了超大集群下 nodes 的计算数量.
-	// 当集群的节点数小于 100 时, 则直接使用集群的节点数作为扫描数据量
-	// 当大于 100 时, 则使用公式计算 `numAllNodes * (50 - numAllNodes/125) / 100`
 	numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes))
 
 	// Create feasible list with enough space to avoid growing it
 	// and allow assigning.
 	feasibleNodes := make([]*v1.Node, numNodesToFind)
 
-	// 如果 framework 未注册 Filter 插件, 则退出.
 	if !fwk.HasFilterPlugins() {
 		for i := range feasibleNodes {
 			feasibleNodes[i] = nodes[(sched.nextStartNodeIndex+i)%numAllNodes].Node()
@@ -529,7 +493,6 @@ func (sched *Scheduler) findNodesThatPassFilters(
 		return feasibleNodes, nil
 	}
 
-	// framework 内置并发控制器, 并发 16 个协程去请求插件的 Filter 方法.
 	errCh := parallelize.NewErrorChannel()
 	var statusesLock sync.Mutex
 	var feasibleNodesLen int32
@@ -538,18 +501,13 @@ func (sched *Scheduler) findNodesThatPassFilters(
 	checkNode := func(i int) {
 		// We check the nodes starting from where we left off in the previous scheduling cycle,
 		// this is to make sure all nodes have the same chance of being examined across pods.
-		// 获取 node info 对象
 		nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes]
-		// 遍历执行 framework 的 Filter 插件的 Filter 方法.
 		status := fwk.RunFilterPluginsWithNominatedPods(ctx, state, pod, nodeInfo)
 		if status.Code() == framework.Error {
-			// 如果有错误, 直接把错误传到 errCh 管道里.
 			errCh.SendErrorWithCancel(status.AsError(), cancel)
 			return
 		}
 		if status.IsSuccess() {
-			// 如果成功执行 Filter 插件的数量超过 numNodesToFind, 则执行 cancel().
-			// 当 ctx 被 cancel(), framework 的并发协程池不会继续执行后续的任务.
 			length := atomic.AddInt32(&feasibleNodesLen, 1)
 			if length > numNodesToFind {
 				cancel()
@@ -576,28 +534,23 @@ func (sched *Scheduler) findNodesThatPassFilters(
 
 	// Stops searching for more nodes once the configured number of feasible nodes
 	// are found.
-	// 并发调用 framework 的 Filter 插件的 Filter 方法.
 	fwk.Parallelizer().Until(ctx, numAllNodes, checkNode, metrics.Filter)
 	feasibleNodes = feasibleNodes[:feasibleNodesLen]
 	if err := errCh.ReceiveError(); err != nil {
-		// 当有错误时, 直接返回.
 		statusCode = framework.Error
 		return feasibleNodes, err
 	}
-	// 返回可用的 nodes 列表.
 	return feasibleNodes, nil
 }
 
 // numFeasibleNodesToFind returns the number of feasible nodes that once found, the scheduler stops
 // its search for more feasible nodes.
 func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) {
-	// 当前的集群小于 100 时, 则直接使用集群节点数作为扫描数
 	if numAllNodes < minFeasibleNodesToFind {
 		return numAllNodes
 	}
 
 	// Use profile percentageOfNodesToScore if it's set. Otherwise, use global percentageOfNodesToScore.
-	// k8s scheduler 的 nodes 百分比默认为 0
 	var percentage int32
 	if percentageOfNodesToScore != nil {
 		percentage = *percentageOfNodesToScore
@@ -607,15 +560,12 @@ func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32,
 
 	if percentage == 0 {
 		percentage = int32(50) - numAllNodes/125
-		// 不能小于5
 		if percentage < minFeasibleNodesPercentageToFind {
 			percentage = minFeasibleNodesPercentageToFind
 		}
 	}
 
-	// 需要扫描的节点数
 	numNodes = numAllNodes * percentage / 100
-	// 如果小于 100, 则使用 100 作为扫描数.
 	if numNodes < minFeasibleNodesToFind {
 		return minFeasibleNodesToFind
 	}
@@ -681,10 +631,6 @@ func findNodesThatPassExtenders(extenders []framework.Extender, pod *v1.Pod, fea
 // The scores from each plugin are added together to make the score for that node, then
 // any extenders are run as well.
 // All scores are finally combined (added) to get the total weighted scores of all nodes
-// prioritizeNodes 方法为调度器的优选阶段的实现.
-// 其内部会遍历调用 framework 的 PreScore 插件集合里 PeScore 方法,
-// 然后再遍历调用 framework 的 Score 插件集合的 Score 方法.
-// 经过 Score 打分计算后可以拿到各个 node 的分值.
 func prioritizeNodes(
 	ctx context.Context,
 	extenders []framework.Extender,
@@ -695,7 +641,6 @@ func prioritizeNodes(
 ) ([]framework.NodePluginScores, error) {
 	// If no priority configs are provided, then all nodes will have a score of one.
 	// This is required to generate the priority list in the required format
-	// 如果 extenders 为空和score 插件为空, 则跳出
 	if len(extenders) == 0 && !fwk.HasScorePlugins() {
 		result := make([]framework.NodePluginScores, 0, len(nodes))
 		for i := range nodes {
@@ -708,15 +653,12 @@ func prioritizeNodes(
 	}
 
 	// Run PreScore plugins.
-	// 在 framework 的 PreScore 插件集合里, 遍历执行插件的 PreSocre 方法
 	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
 	if !preScoreStatus.IsSuccess() {
-		// 只有有异常直接退出
 		return nil, preScoreStatus.AsError()
 	}
 
 	// Run the Score plugins.
-	// 在 framework 的 Score 插件集合里, 遍历执行插件的 Socre 方法
 	nodesScores, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
 	if !scoreStatus.IsSuccess() {
 		return nil, scoreStatus.AsError()
@@ -726,7 +668,6 @@ func prioritizeNodes(
 	klogV := klog.V(10)
 	if klogV.Enabled() {
 		for _, nodeScore := range nodesScores {
-			// 打印插件名字和分值 score
 			for _, pluginScore := range nodeScore.Scores {
 				klogV.InfoS("Plugin scored node for pod", "pod", klog.KObj(pod), "plugin", pluginScore.Name, "node", nodeScore.Name, "score", pluginScore.Score)
 			}
@@ -736,7 +677,6 @@ func prioritizeNodes(
 	if len(extenders) != 0 && nodes != nil {
 		// allNodeExtendersScores has all extenders scores for all nodes.
 		// It is keyed with node name.
-		// 当额外 extenders 调度器不为空时, 则需要计算分值.
 		allNodeExtendersScores := make(map[string]*framework.NodePluginScores, len(nodes))
 		var mu sync.Mutex
 		var wg sync.WaitGroup
@@ -806,25 +746,19 @@ func prioritizeNodes(
 
 // selectHost takes a prioritized list of nodes and then picks one
 // in a reservoir sampling manner from the nodes that had the highest score.
-// selectHost 是从优选的 nodes 集合里获取分值 socre 最高的 node. 内部还做了一个小优化, 当相近的两个 node 分值相同时, 则通过随机来选择 node, 目的使 k8s node 的负载更趋于均衡
 func selectHost(nodeScores []framework.NodePluginScores) (string, error) {
-	// 如果 nodes 为空, 则返回错误
 	if len(nodeScores) == 0 {
 		return "", fmt.Errorf("empty priorityList")
 	}
-	// 直接从头到位遍历 nodeScores 数组, 拿到分值 score 最后的 nodeName.
 	maxScore := nodeScores[0].TotalScore
 	selected := nodeScores[0].Name
 	cntOfMaxScore := 1
 	for _, ns := range nodeScores[1:] {
 		if ns.TotalScore > maxScore {
-			// 当前的分值更大, 则进行赋值.
 			maxScore = ns.TotalScore
 			selected = ns.Name
 			cntOfMaxScore = 1
 		} else if ns.TotalScore == maxScore {
-			// 当两个 node 的 分值相同时,
-			// 使用随机算法来选择当前和上一个 node.
 			cntOfMaxScore++
 			if rand.Intn(cntOfMaxScore) == 0 {
 				// Replace the candidate with probability of 1/cntOfMaxScore
@@ -832,7 +766,6 @@ func selectHost(nodeScores []framework.NodePluginScores) (string, error) {
 			}
 		}
 	}
-	// 返回分值最高的 node
 	return selected, nil
 }
 
@@ -909,10 +842,6 @@ func getAttemptsLabel(p *framework.QueuedPodInfo) string {
 
 // handleSchedulingFailure records an event for the pod that indicates the
 // pod has failed to schedule. Also, update the pod condition and nominated node name if set.
-// 如何处理调度失败的 pod
-// 前面有说 kubernetes scheduler 的 scheduleOne 作为主循环处理函数，当没有为 pod 找到合适 node 时，会调用 FailureHandler 方法.
-// FailureHandler() 是由 handleSchedulingFailure() 方法实现. 该逻辑的实现简单说就是把失败的 pod 扔到 podBackoffQ 队列或者 unschedulablePods 集合里.
-
 func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framework.Framework, podInfo *framework.QueuedPodInfo, status *framework.Status, nominatingInfo *framework.NominatingInfo, start time.Time) {
 	reason := v1.PodReasonSchedulerError
 	if status.IsUnschedulable() {
@@ -969,7 +898,6 @@ func (sched *Scheduler) handleSchedulingFailure(ctx context.Context, fwk framewo
 			// ignore this err since apiserver doesn't properly validate affinity terms
 			// and we can't fix the validation for backwards compatibility.
 			podInfo.PodInfo, _ = framework.NewPodInfo(cachedPod.DeepCopy())
-			// 重新入队列
 			if err := sched.SchedulingQueue.AddUnschedulableIfNotPresent(podInfo, sched.SchedulingQueue.SchedulingCycle()); err != nil {
 				klog.ErrorS(err, "Error occurred")
 			}
diff --git a/pkg/scheduler/scheduler.go b/pkg/scheduler/scheduler.go
index 5eb9dd2fb35..540d46f038b 100644
--- a/pkg/scheduler/scheduler.go
+++ b/pkg/scheduler/scheduler.go
@@ -239,7 +239,6 @@ var defaultSchedulerOptions = schedulerOptions{
 }
 
 // New returns a Scheduler
-// 实例化 kubernetes scheduler 对象, 初始化流程
 func New(client clientset.Interface,
 	informerFactory informers.SharedInformerFactory,
 	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
@@ -267,7 +266,6 @@ func New(client clientset.Interface,
 		options.profiles = cfg.Profiles
 	}
 
-	// 构建 registry 对象，插件注册
 	registry := frameworkplugins.NewInTreeRegistry()
 	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
 		return nil, err
@@ -287,7 +285,6 @@ func New(client clientset.Interface,
 	clusterEventMap := make(map[framework.ClusterEvent]sets.String)
 	metricsRecorder := metrics.NewMetricsAsyncRecorder(1000, time.Second, stopCh)
 
-	// profiles 用来保存不通调度器的 framework 框架，framework 则用来存放 plugin 。
 	profiles, err := profile.NewMap(options.profiles, registry, recorderFactory, stopCh,
 		frameworkruntime.WithComponentConfigVersion(options.componentConfigVersion),
 		frameworkruntime.WithClientSet(client),
@@ -313,7 +310,6 @@ func New(client clientset.Interface,
 	for profileName, profile := range profiles {
 		preEnqueuePluginMap[profileName] = profile.PreEnqueuePlugins()
 	}
-	// 实例化 podQueue 队列, 该队列为 PriorityQueue
 	podQueue := internalqueue.NewSchedulingQueue(
 		profiles[options.profiles[0].SchedulerName].QueueSortFunc(),
 		informerFactory,
@@ -331,36 +327,31 @@ func New(client clientset.Interface,
 		fwk.SetPodNominator(podQueue)
 	}
 
-	// 实例化 Cache 缓存
 	schedulerCache := internalcache.New(durationToExpireAssumedPod, stopEverything)
 
 	// Setup cache debugger.
 	debugger := cachedebugger.New(nodeLister, podLister, schedulerCache, podQueue)
 	debugger.ListenForSignal(stopEverything)
 
-	// 实例化 scheduler 对象
 	sched := &Scheduler{
 		Cache:                    schedulerCache,
 		client:                   client,
 		nodeInfoSnapshot:         snapshot,
 		percentageOfNodesToScore: options.percentageOfNodesToScore,
 		Extenders:                extenders,
-		// NextPod 底层引用了 MakeNextPodFunc 方法, 其内部从 PriorityQueue 队列中获取 pod 对象.
-		NextPod:         internalqueue.MakeNextPodFunc(podQueue),
-		StopEverything:  stopEverything,
-		SchedulingQueue: podQueue,
-		Profiles:        profiles,
+		NextPod:                  internalqueue.MakeNextPodFunc(podQueue),
+		StopEverything:           stopEverything,
+		SchedulingQueue:          podQueue,
+		Profiles:                 profiles,
 	}
 	sched.applyDefaultHandlers()
 
-	// 在 informer 里注册自定义的事件处理方法
 	addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))
 
 	return sched, nil
 }
 
 // Run begins watching and scheduling. It starts scheduling and blocked until the context is done.
-// scheudler 启动入口
 func (sched *Scheduler) Run(ctx context.Context) {
 	sched.SchedulingQueue.Run()
 
@@ -370,9 +361,6 @@ func (sched *Scheduler) Run(ctx context.Context) {
 	// If there are no new pods to schedule, it will be hanging there
 	// and if done in this goroutine it will be blocking closing
 	// SchedulingQueue, in effect causing a deadlock on shutdown.
-	// 	Run() 方法是 k8s scheduler 的启动运行入口, 其流程是先启动 queue 队列的 Run 方法, 再异步启动一个协程处理核心调度方法 scheduleOne.
-	// schedulingQueue 的 Run() 方法用来监听内部的延迟任务, 把到期的任务放到 activeQ 中.
-	// 而 scheduleOne 方法用来从优先级队列里获取由 informer 插入的 pod 对象, 调用 schedulingCycle 为 pod 选择最优的 node 节点. 如果找到了合适的 node 节点, 则调用 bindingCycle 方法来发起 pod 和 node 绑定.
 	go wait.UntilWithContext(ctx, sched.scheduleOne, 0)
 
 	<-ctx.Done()
